= Integration

== The Challenge

The task that you are working on will be to enable your robot to identify a fedora on the floor. Your robot should drive towards the fedora and stop shortly before it.

The basic flow for your app will be:

* retrieving an image
* sending it to the object detection service
* analyzing the results
** detected class
** confidence score
** coordinates of the bounding boxes in your camera image
* working with the result to determine where the robot should move to next

== MVP (Minimal Viable Product)

To speed things up we will give you a solution as an MVP.

Change your code to this :

[source,python,role=execute]
----
# Drop your code here
turn_counter = 0
while thread_event.is_set():
    objects = take_picture_and_detect_objects()
    coordinates = find_highest_score(objects)

    if coordinates and coordinates.confidence_score > 0.5:
        print(f'''Object with highest score -> [
            confidence score: {coordinates.confidence_score},
            x upper left corner: {coordinates.x_upper_left},
            y upper left corner: {coordinates.y_upper_left},
            x lower right corner: {coordinates.x_lower_right},
            y lower right corner: {coordinates.y_lower_right},
            object class: {coordinates.object_class} ]''')

        move_x = (coordinates.x_upper_left + coordinates.x_lower_right) / 2
        print(f'move_x: {move_x}')
        if move_x < 320:
            turn_left(10)
        else:
            turn_right(10)

        delta = coordinates.x_lower_right - coordinates.x_upper_left
        print(f'delta: {delta}')
        if delta < 350:
            move_forward(10)
        else:
            print('Done - arrived at the object')
            return

    else:
        if turn_counter < 360:
            turn_left(10)
            turn_counter = turn_counter + 10
        else:
            print('Done - no object found')
            return

print('Done')
----

Now place a Fedora somewhere near your robot and see if it can find it.

== Dev & AI Collaboration

Have a look at this code. Perhaps there is something you can optimize?

Think about these points:

* How can I align the robot towards the fedora with the coordinate information?
* How do I know when to stop the robot?
* How can I optimize the "think" and "react" phases?
* How can I handle corner cases?
* Perhaps the model should be tuned to increase detection results?

== Feature Freeze
That's it. You now have a fully autonomous robot that can help you find your lost Fedora.

This was actually the short version of our Robo Vision AI Workshop. There is a longer version, where you will build and deploy your app and model directly to the robot. It can then run completely disconnected as an autonomous Edge Unit. Reach out to your Red Hat team if you are interested.