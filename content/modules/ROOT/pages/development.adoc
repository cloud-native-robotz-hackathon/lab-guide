= Development

Now that your AI model is ready and serving predictions, let's build the app that will use it to control the robot!

== Start Coding in Dev Spaces

To develop our flashy AI enhanced app, we will use **OpenShift Dev Spaces**, a web IDE that comes with every OpenShift installation. No need to setup IDEs, runtimes and tools on your laptop.


[NOTE]
====
Most of the applications you are about to use are enabled for Single Sign-On using **Red Hat build of Keycloak**. So when you first access the OpenShift Console, you'll be directed to Keycloak for authentication.
====

* If you are not already logged in, login into the {openshift_console_url}[OpenShift Console,window=_blank] with:
** username:
+
[source,text,role=execute,subs="attributes"]
----
{user}
----
** password :
+
[source,text,role=execute,subs="attributes"]
----
{password}
----

* In the OpenShift Console on the top right, click on the square icon and select **Red Hat OpenShift Dev Spaces**
* Login with your OpenShift credentials
* Give permissions if required

Next we are going to start a new Workspace where all the editing is done and we will initialize it with a https://devfile.io/[DevFile].

TIP: A **DevFile** is a definition for environment, tooling, settings and runtimes for our development Workspace. No more endless installations and configurations. We will be ready to hack right away. Since a matching **DevFile** is already checked into our Git repo, we can use it directly to initialize a workspace.

* Select **Create Workspace** in the left menu
* Enter **Git Repo URL** :
+
[source,bash,role=execute,subs="attributes"]
----
{gitea_console_url}/{user}/starter-app-python/raw/branch/master/devfile.yaml
----

TIP: You could also find the URL to this DevFile manually by opening your {gitea_console_url}[Gitea,window=_blank], navigate to the repo `starter-app-python` and then `devfile.yaml`. Click on "Original version / Raw" and copy the URL.

* Click on **Create and Open**

image::devspaces-create-workspace.png[]

Wait for the Workspace to deploy (this may take some time). Once your Workspace is up and running, you should see a view that looks familiar if you have used VSCode before.

We have prepared some sample code that you can use to quickly get started with your app.

* If you see a Popup asking you to "Do you trust the authors", click check **Always ...**  and click **Yes ...**

image::vscode-trust.png[]

Next we will clone the starter code for our robot control app into our Workspace.

* On the left side click on the button **Clone Repository**
* In the form at the top enter the git url
+
[source,bash,role=execute,subs="attributes"]
----
{gitea_console_url}/{user}/starter-app-python.git
----
* Hit **Enter**
* In the next form keep the path as is ( `/projects` ) and hit **Enter** again
* In the next dialog click on the **Open** button to reopen the workspace in this directory

You will now see the starter code on the left side.

This is a Python application that is served as a REST API server through the Gunicorn framework. You will call the endpoints through a website and the app in turn will call the endpoints of the Robot.

=== Start the App

To start the app, you can use a task that is defined in the **DevFile** and runs all the pip and Python commands under the hood.

* In the top left, click on the "hamburger" menu > **Terminal** > **Run Task ...**
* In the selection menu at the top, click on **devfile**
* Then click on **devfile: Run the application**

This will install the python dependencies and start the server of your app.

=== Open the Robot Control Page

TIP: Once the Python Gunicorn server has started, you will see two Popup windows in the bottom right corner that you will need to approve. These will setup a port forwarding and publish a **Route** in OpenShift through which you can reach the website of your app running the Workspace.

* Click on **Yes**:
+
image::devspaces-allow_route.png[]

* Click on **Open in new tab**
+
image::devspaces-open-new-tab.png[]

* Click on **Open**
+
image::devspaces-open-tab.png[]

A new browser tab with the web interface of the Robot Control app will open. Make sure you are on `http://` if the page does not open.

=== If you missed the Popups

.Click here to restart the Control Page
[%collapsible]
====

**If you have missed any of the Popups, you need to end and restart the process:**

* Click on the garbage can icon on the right of the terminal

image::kill-terminal.png[]

* Now restart the Gunicorn server with the task as explained above

If you have missed the second of the Popups (opening the Route), there is another way to open it:

* In the left pane at the bottom expand the **ENDPOINTS** section
* Hover over the entry `user-port-forward(8080)`
* Click in the icon (`Open in new Tab`)
* A tab with the Robot Control App will open in your Browser

image::open-endpoints.png[]

====

=== Robot Control Page Overview

This webpage has a few buttons that essentially just call REST endpoints in your app.  You will use it to start your robot control code.   The website features an **Initiate Run** button that you will use to execute your code.

The buttons:

* **Check Status** - Calls the app to see if the robot is connected
* **Initiate Run** - Calls your `startRobot()` function in the `app.py`. This is where you will add your code
* **Emergency Stop** - Stop execution of your app

Check if everything is setup correctly and your app can reach the robot:

* The *Live Visual Feed* will be empty. Don't worry, we will get to that later.
* On the Robot Control Page click on **Check Status**

image::robot-control-page2.png[]

=== The Robot API

Now that our app is running, let's take a moment to have a look at the Robot REST API that we will be calling.

It is fairly simple:

* POST /forward/<length>
** Drive forward by length
* POST /backward/<length>
** Drive backwards by length
* POST /left/<degrees>
** Turn left by degrees
* POST /right/<degrees>
** Turn right by degrees
* GET /image
** Returns a base64 image of the current camera image
* GET /status
** Returns the status of the robot
* GET /power
** Returns the current power of the robot

For testing purposes you can call the Robot API directly from your Workspace Terminal with `curl`.

* Open a new Terminal with the **+** Icon at the top right of the current Gunicorn Terminal
* To drive forward 2 units (make sure your Robot is not falling off the table), enter an execute:

[source,bash,role=execute]
----
include::partial$export-{user}.adoc[]

curl -X POST http://hub-controller-live.red-hat-service-interconnect-data-center.svc.cluster.local:8080/robot/forward/2 -d user_key=${ROBOT_NAME}

----

TIP: Some browsers will ask you whether to allow pasting text. Approve this to proceed.

Awesome, right? You are now an official robot pilot.

To retrieve a camera image and save it as a file:

[source,bash,role=execute]
----
include::partial$export-{user}.adoc[]

curl -v http://hub-controller-live.red-hat-service-interconnect-data-center.svc.cluster.local:8080/robot/camera?user_key=${ROBOT_NAME} | base64 -d > image.jpg
----

You will find the image file as `image.jpg` in the root folder. Click on it to view.

That was neat, but of course you want to give the robot some personality with your code. Let's move on to configure the app.

=== Connecting the App to Endpoints

Our app is running in a **Dev Spaces** container. We need to call the Robot API as well as the Inferencing API to do our coding magic.

So let's go ahead and set up these connection variables. Edit the file `config.py` to add your inferencing endpoint that you created in the previous chapter.

Replace the following placeholders:




* **<REPLACE_WITH_ROBOT_NAME>**:
+
[source,bash,role=execute,subs="attributes"]
----
include::partial${user}.adoc[]
----
* **<REPLACE_WITH_INFERENCING_API>**
+
(The Object Detection Service from the DataScience chapter)
* **<REPLACE_WITH_INFERENCING_API_TOKEN>**
+
(The Token of the Object Detection Service from the DataScience chapter)


TIP: Note that **Dev Spaces** saves your file directly while you edit it. No need to save manually. And as an added bonus gunicorn reloads your python app, so there is also no need to restart your app or even reload the Robot control page.



=== First Code: Robot Movement

To make things a bit easier some helper functions are already in place, for example to create REST requests for the robot operations and to work with the array response coming from the inferencing service.

Let's write some code and drive our robot, but this time from our code:

* Open the file `app.py`
* Look for the function `startRobot()` and the comment `# Drop your code here`
* Add a `move_forward()` function call with 10 units, so your code looks like this (watch out for Python indentation):

[source,bash,role=execute,subs="attributes"]
----
# Drop your code here
move_forward(10)
print('Done')
----

* Now back on your Robot Control Page click on the **Initiate Run** button

If all goes well your robot should move forward. Congratulations, your robot has gained a bit of consciousness! OBS, do make sure the robot is not on the edge of a table or in risk of falling down.

If it doesn't move, have a look at the Terminal output in **Dev Spaces** and recheck your `config.py` entries.

TIP: Sometimes the auto-reload of the app doesn't work and the app stops, you'll see this in the terminal window. If this happens, just follow the steps above under **If you missed the Popups** and the app will restart.

=== Some more movement

Now let the robot drive forward, look left and right and then retreat again.

Edit the function `startRobot()` again so it looks like this:

[source,python,role=execute,subs="attributes"]
----
# Drop your code here
move_forward(10)
turn_left(90)
turn_right(180)
turn_left(90)
move_backward(10)
print('Done')
----

Run the code by clicking the **Initiate Run** button.

Wow, almost a robot ballet there.

=== Robo Vision

Now comes the exciting part! Let's give your robot vision by connecting it to your AI model.

We have two helper functions that will help us:

* `take_picture_and_detect_objects()` - gets a camera image, runs it through the inferencing service and returns an array of detected objects
* `find_highest_score(objects)` - returns the object with the highest confidence score

The `object` itself will have these fields that you can use:

* `object class` - what object class was detected (will be 0 for a fedora)
* `confidence_score` - How certain is the inferencing service that this is actually the detected object (the higher the better)
* `x_upper_left` - Bounding box upper left corner x coordinate
* `y_upper_left` - Bounding box upper left corner y coordinate
* `x_lower_right` - Bounding box lower right corner x coordinate
* `y_lower_right` - Bounding box lower left corner y coordinate

Let's test the AI vision! Change your code to detect an object through the camera:

[source,python,role=execute,subs="attributes"]
----
# Drop your code here
objects = take_picture_and_detect_objects()
coordinates = find_highest_score(objects)

if coordinates:
    print(f'''Object with highest score -> [
        confidence score: {coordinates.confidence_score},
        x upper left corner: {coordinates.x_upper_left},
        y upper left corner: {coordinates.y_upper_left},
        x lower right corner: {coordinates.x_lower_right},
        y lower right corner: {coordinates.y_lower_right},
        object class: {coordinates.object_class} ]''')
else:
    print('No objects found')
print('Done')
----

* Now place some objects in front of the camera and execute your code by pressing the **Initiate Run** button
* Have a look at the Console output in your Dev Spaces Workspace
* Place a fedora in front of the camera, run your code again and see if that makes a difference:

You should now see an output similar to this:

----
Object with highest score -> [
            confidence score: 0.8367316560932,
            x upper left corner: 296.75372999999996,
            y upper left corner: 321.65746,
            x lower right corner: 515.7144099999999,
            y lower right corner: 477.20844,
            object class: 0.0 ]
----

ðŸ¤– **Awesome!** Your AI just detected a Fedora (object class = 0) with a confidence score of 0.8! Your robot can now see!

The robot control page will also display the last image from the camera every 5 seconds. You can use this to check the robot and camera alignment.

=== Ready

ðŸŽ¯ Perfect! You now have all the tools required to create a fedora seeking robot:
* âœ… Robot movement controls
* âœ… AI vision integration
* âœ… Object detection working

Head on over to the next chapter to put it all together and make your robot autonomous!






